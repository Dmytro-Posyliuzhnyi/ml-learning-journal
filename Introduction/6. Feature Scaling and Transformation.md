
# Feature Scaling and Transformation

When a feature distribution has a *heavy tail* (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization, will squash most values into a small range. Machine learning models generally don't like this at all. So *before* you scale the feature, you should first tranform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as *power law distribution*, then replacing the feature with its logarithm may help. For example, the *population* feature in dataset used in the following [implementation](https://github.com/Dmytro-Posyliuzhnyi/ml-learning-journal/blob/main/Introduction/Code/Hands-On%20ML/dataset.py) roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. Below you can see how feature looks after computing log of its values. It seems very close to a Gaussian distribution (i.e., bell-shaped):

```python
transformed_population = np.log(housing['population'])
visualize_feature_transformation(housing, 'population', transformed_population)
```

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/15b1378f-ff64-47a4-bbed-1f17f2b94060">

Another approach to handle heavy-tailed features consists in *bucketizing* the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to. In many cases, binning turns numerical data into categorical data. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there's no need for further scaling, or you can just divide by the number of buckets to force the values to the 0-1 range.

For example, consider a feature named X whose lowest value is 15 and highest value is 425. Using binning, you could represent X with the following five bins:

- **Bin 1**: 15 to 34
- **Bin 2**: 35 to 117
- **Bin 3**: 118 to 279
- **Bin 4**: 280 to 392
- **Bin 5**: 393 to 425

Bin 1 spans the range 15 to 34, so every value of X between 15 and 34 ends up in Bin 1. A model trained on these bins will react no differently to X values of 17 and 29 since both values are in Bin 1.

Even though X is a single column in the dataset, binning causes a model to treat X as five separate features. Therefore, the model learns separate weights for each bin.

Binning can feel counterintuitive, given that the model in the previous example treats the values 37 and 115 identically. But when a feature appears more clumpy than linear, binning is a much better way to represent the data.
