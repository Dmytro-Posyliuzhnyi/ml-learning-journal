
# Feature Scaling and Transformation

When a feature distribution has a *heavy tail* (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization, will squash most values into a small range. Machine learning models generally don't like this at all. So *before* you scale the feature, you should first tranform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as *power law distribution*, then replacing the feature with its logarithm may help. For example, the *population* feature in dataset used in the following [implementation](https://github.com/Dmytro-Posyliuzhnyi/ml-learning-journal/blob/main/Introduction/Code/Hands-On%20ML/dataset.py) roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. Below you can see how feature looks after computing log of its values. It seems very close to a Gaussian distribution (i.e., bell-shaped):

```python
transformed_population = np.log(housing['population'])
visualize_feature_transformation(housing, 'population', transformed_population)
```

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/15b1378f-ff64-47a4-bbed-1f17f2b94060">

Another approach to handle heavy-tailed features consists in *bucketizing* the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to. In many cases, binning turns numerical data into categorical data. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there's no need for further scaling, or you can just divide by the number of buckets to force the values to the 0-1 range.

For example, consider a feature named X whose lowest value is 15 and highest value is 425. Using binning, you could represent X with the following five bins:

- **Bin 1**: 15 to 34
- **Bin 2**: 35 to 117
- **Bin 3**: 118 to 279
- **Bin 4**: 280 to 392
- **Bin 5**: 393 to 425

Bin 1 spans the range 15 to 34, so every value of X between 15 and 34 ends up in Bin 1. A model trained on these bins will react no differently to X values of 17 and 29 since both values are in Bin 1.

Even though X is a single column in the dataset, binning causes a model to treat X as five separate features. Therefore, the model learns separate weights for each bin.

Binning can feel counterintuitive, given that the model in the previous example treats the values 37 and 115 identically. But when a feature appears more clumpy than linear, binning is a much better way to represent the data.

We can also have a multimodal distribution (i.e. with two or more clear peaks, called modes), such as the *housing_median_age* feature (from dataset used in this [implementation](https://github.com/Dmytro-Posyliuzhnyi/ml-learning-journal/blob/main/Introduction/Code/Hands-On%20ML/dataset.py)). 

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/8e8d6945-501f-431f-8581-704d778495c9">

One of the approaches here would be to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a *radial basis function (RBF)* - any function that depends only on the distance between the input value and a fixed point.

**So as I understand** instead of using the raw feature values (like the actual age), we replace the feature with multiple new features. Each new feature represents the similarity of the data point to one of the modes.
So if we have a house age feature, and we've identified three modes in the data: mode 1 (ages 0-20 years), mode 2 (ages 21-50 years), and mode 3 (ages 51+ years). If a particular house is 25 years old, instead of just passing the value "25" to the model, the model would receive similarity scores, such as:
  - Similarity to mode 1: 0.2
  - Similarity to mode 2: 0.8
  - Similarity to mode 3: 0.1

These new similarity features are what the model will use to make predictions.

The most commonly used RBF is the *Gaussian RBF*, whose output value decays exponentially as the input value moves away from the fixed point. For example, the Gaussian RBF similarity between the housing age $`x`$ and 35 is given by the equation $`exp(-y(x-35)^2)`$. The hyperparameter $`y`$ (gamma) determines how quickly the similarity measure decays as $`{x}`$ moves away from 35. Using Scikit-Learn's `rbf_kernel()` function, we can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:

```python
from sklearn.metrics.pairwise import rbf_kernel
age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)
```

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/ef23abe7-7569-4c0c-b23a-f282cf7ff370">

