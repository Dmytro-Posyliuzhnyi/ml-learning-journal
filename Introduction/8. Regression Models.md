
# Polynomial Regression

In case if the data is more complex than a straight line, we still can use a linear model to fir non-linear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called **polynomial regression**. 

Let's take a look at this non-linear and noisy dataset:

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/a504bf66-e128-4df3-a1dc-74990e00ddbd">

```python
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)
```

Clearly, straight line will never fit this data properly. We can use *scikit-learn* *Polynomial Features* class to transform the training data, adding square (second-degree polynomial) of each feature in the training set as a new feature (in this case there is just one feature):

```python
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
```

Now the *X_poly* variable contains the original feature value and the square of this value. We can use this variable with *Linear Regression* class from *scikit-learn* and see that estimations are quite close to the original function:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
```

This model estimates $`\hat{y}=0.56x_1^2 + 0.93x_1 + 1.78`$ when in fact the original function was: $`y=0.5x_1^2 + 1.0x_1 + 2.0 + \text{Gaussian noise}`$

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/21000194-f334-4fb9-8247-633a3e9441f9">

Polynomial regression is also capable of finding the relationships between features, which is something a plain linear regression model cannot do. This is made possible by the fact that *PolynomialFeatures* also adds all combinations of features up to the given degree. For example if there were two features *a* and *b*, *PolynomialFeatures* with degree 3 would not onl add the features $a^2$, $`a^2`$, $`b^2`$, $`b^3`$, but also the combinations $`ab`$, $`a^2b`$, and $`ab^2`$. We need to be careful of the combinatorial explosion that can occur when the number of features is large.

---

# Learning Curve

| Distinguish between overfitting and underfitting |
|--------------------------------------------------|
| If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then the model is overfitting. If it performs poorly on both the training data and the cross-validation metrics, then the model is underfitting. This is one way to tell when a model is too simple or too complex. |

Another way to tell whether model is too simple or too complex is to look at *learning curves*, which are plots of the model's training error and validation error as a function of the training iteration: just evaluate the model at regular intervals during training on both the training set and the validation set, and plot the results.

Here is an example of the *learning curve*:

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/5eccc112-0a94-4161-a221-2a08551c7a0d">

When there are just couple of instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the training data perfectly, both because the data is noisy and because the model is not linearat all. So the error on the training set goes up until it reaches a plateu, at which point adding new instances to the training set doesn't make the average error much better or worse. On the contrary, when the model is trained on a very few instances, it is incapable of generalizing properly, which is why the validation error is initially quite large. Then, as the model is shown more training examples, it learns, and thus the validation error slowly goes down. However, once again a straight line cannot do a good job of modeling the data, so the error ends up at a plateau, very close to the other curve. These learning curves are typical of a model that's underfitting. Both curves have reached a plateau; they are close and fairly high.

| About underfitting |
|--------------------------------------------------|
| If the model is underfitting the training data, adding more training examples will most likely not help. You need to use a better model or come up with better features |

Now let's look at another example where I've adjusted the model according to the book, making it a polynomial regression (10th-degree):

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/6e1da4ce-9246-4abc-b1ee-477e01a8cb19">

Two important differences between this curve and the previous one:

- The error on the training data is much lower
- The gap between curves. The model performs better on the training data than on the validation data, which is the hallmark of an overfitting model. If you used a much larger training set, however, the two curves would continue to get closer.

| About overfitting |
|--------------------------------------------------|
| One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error |

---

# Regularized Linear Models

A good way to reduce overfitting is to regularize the model: the fewer degrees of freedom it has, the harder it will be for it to overfit the data. A simple way to regularize a polynomial model is to reduce the number of polynomial degrees. For a linear model, regularization is typically achieved by constraining the weights of the model. We will explore **ridge regression**, **lasso regression** and **elastic net regression**, which implement three different ways to constrain the weights.

## Ridge Regression

Ridge Regression is a type of linear regression that includes **L2 regularization** to prevent overfitting by discouraging large weights.

### What is Ridge Regression?

In standard linear regression, we minimize the following function:
$`\frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2`$

- $`y_i`$: Actual value
- $`\hat{y}_i`$: Predicted value
- $`n`$: Number of samples

Ridge Regression adds a penalty term to this function to shrink the weights:
$`\frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 + \lambda \sum w_j^2`$

- $`\lambda`$: Regularization strength (a hyperparameter)
- $`\sum w_j^2`$: The sum of the squared weights (L2 norm)

Here is a nice visualization from the book:

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/8db128b2-bff6-4b90-a32a-df8ea62ea05c">

On this image we can see several ridge models that were trained on some very noisy linear data using different regularization strength values (the book refers to it as $`\alpha`$). On the left, plain ridge models are used, leadin to linear predictions. On the right the data is first expanded using *PolynomialFeatures(degree=10)*, then it is scaled using *StandartScaler*, and finally the ridge models are applied to the resulting features: this is polynomial regression with ridge regularization. Note how increasing $`\alpha`$ leads to flatter (i.e., less extreme, more reasonable) predictions, thus reducing model's variance but increasing its bias.

| Before ridge regression |
|--------------------------------------------------|
| It is important to scale the data (e.g. using a *StandartScaler*) before performing ridge regression, as it is sensitive to the scale of the input features. This is true of most regularized models. |


### My intuition behind the L2 regularization


| Weight ($`w`$) | Value | Squared Value ($`w^2`$) |
|-----------------|-------|-------------------------|
| $`w_1`$        | 0.2   | 0.04                    |
| $`w_2`$        | -0.5  | 0.25                    |
| $`w_3`$        | 5.0   | 25.00                   |
| $`w_4`$        | -1.2  | 1.44                    |
| $`w_5`$        | 0.3   | 0.09                    |
| $`w_6`$        | -0.1  | 0.01                    |
| **Total**       |       | **26.83**              |

On the following table you can see that weights which are quite small don't affect the regularization much, but large weights can have a huge impact. For example:
- A single weight ($`w_3`$) contributes about 93% of the total complexity.
- The other five weights (which have range from -1.2 to 0.3) collectively contribute only about 7% of the total complexity.

## Lasso Regression

Lasso Regression is a type of linear regression that uses **L1 regularization** to prevent overfitting by penalizing the absolute values of the model's weights. It is especially useful when you have many features, as it can shrink some weights to exactly zero, so basically performing a **feature selection**.

In standard linear regression, the model minimizes the following function:
$`\frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2`$

Lasso Regression adds an **L1 penalty** to this:
$`\frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 + \lambda \sum |w_i|`$

- $`\lambda`$: Regularization strength (a hyperparameter).
- $`\sum |w_i|`$: The sum of the absolute values of the weights.
- Larger $`\lambda`$ increases the penalty, shrinking more weights to zero.

I still did not grasped the mathematical inuition behind this, but as I understand one of the important distinctions is that **Lasso Regression** can perform a **feature selection** what cannot be done with **ridge regression**.

## ElasticNet Regression

*ElasticNet Regression* is a middle ground between ridge regression and lasso regression.

The function for Elastic Net Regression is:

$`\frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 + \lambda \cdot \left( \alpha \sum |w_i| + (1 - \alpha) \sum w_i^2 \right)`$

Where:
- $`\frac{1}{n} \sum \left( y_i - \hat{y}_i \right)^2`$: Mean Squared Error (MSE).
- $`\lambda`$: Regularization strength.


- $`\alpha`$: L1 ratio (controls the mix of L1 and L2 regularization):
  - $`\alpha = 1`$: Equivalent to Lasso (L1 only).
  - $`\alpha = 0`$: Equivalent to Ridge (L2 only).
  - $`0 < \alpha < 1`$: Balance between L1 and L2.
- $`\sum |w_i|`$: L1 penalty (sum of absolute weights).
- $`\sum w_i^2`$: L2 penalty (sum of squared weights).

| Which algortihm to use |
|--------------------------------------------------|
| From what I've read in the book (but haven't proved it on practice yet), almost always preferable to have at least a little bit of regularization and in most cases avoid linear regression. *Ridge* is a good default, but if you suspect that only a few features are useful, you should prefer *lasso* or *elastic net* because they tend to reduce the useless features weights down to zero, as discussed earlier. In general, *elastic net* is preferred over *lasso* because *lasso* may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.  |
