
# Polynomial Regression

In case if the data is more complex than a straight line, we still can use a linear model to fir non-linear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called **polynomial regression**. 

Let's take a look at this non-linear and noisy dataset:

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/a504bf66-e128-4df3-a1dc-74990e00ddbd">

```python
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)
```

Clearly, straight line will never fit this data properly. We can use *scikit-learn* *Polynomial Features* class to transform the training data, adding square (second-degree polynomial) of each feature in the training set as a new feature (in this case there is just one feature):

```python
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
```

Now the *X_poly* variable contains the original feature value and the square of this value. We can use this variable with *Linear Regression* class from *scikit-learn* and see that estimations are quite close to the original function:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
```

This model estimates $`\hat{y}=0.56x_1^2 + 0.93x_1 + 1.78`$ when in fact the original function was: $`y=0.5x_1^2 + 1.0x_1 + 2.0 + \text{Gaussian noise}`$

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/21000194-f334-4fb9-8247-633a3e9441f9">

Polynomial regression is also capable of finding the relationships between features, which is something a plain linear regression model cannot do. This is made possible by the fact that *PolynomialFeatures* also adds all combinations of features up to the given degree. For example if there were two features *a* and *b*, *PolynomialFeatures* with degree 3 would not onl add the features $a^2$, $`a^2`$, $`b^2`$, $`b^3`$, but also the combinations $`ab`$, $`a^2b`$, and $`ab^2`$. We need to be careful of the combinatorial explosion that can occur when the number of features is large.

---

# Learning Curve

| Distinguish between overfitting and underfitting |
|--------------------------------------------------|
| If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then the model is overfitting. If it performs poorly on both the training data and the cross-validation metrics, then the model is underfitting. This is one way to tell when a model is too simple or too complex. |

Another way to tell whether model is too simple or too complex is to look at *learning curves*, which are plots of the model's training error and validation error as a function of the training iteration: just evaluate the model at regular intervals during training on both the training set and the validation set, and plot the results.

Here is an example of the *learning curve*:

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/5eccc112-0a94-4161-a221-2a08551c7a0d">

When there are just couple of instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the training data perfectly, both because the data is noisy and because the model is not linearat all. So the error on the training set goes up until it reaches a plateu, at which point adding new instances to the training set doesn't make the average error much better or worse. On the contrary, when the model is trained on a very few instances, it is incapable of generalizing properly, which is why the validation error is initially quite large. Then, as the model is shown more training examples, it learns, and thus the validation error slowly goes down. However, once again a straight line cannot do a good job of modeling the data, so the error ends up at a plateau, very close to the other curve. These learning curves are typical of a model that's underfitting. Both curves have reached a plateau; they are close and fairly high.

| About underfitting |
|--------------------------------------------------|
| If the model is underfitting the training data, adding more training examples will most likely not help. You need to use a better model or come up with better features |

Now let's look at another example where I've adjusted the model according to the book, making it a polynomial regression (10th-degree):

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/6e1da4ce-9246-4abc-b1ee-477e01a8cb19">

Two important differences between this curve and the previous one:

- The error on the training data is much lower
- The gap between curves. The model performs better on the training data than on the validation data, which is the hallmark of an overfitting model. If you used a much larger training set, however, the two curves would continue to get closer.

| About overfitting |
|--------------------------------------------------|
| One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error |


