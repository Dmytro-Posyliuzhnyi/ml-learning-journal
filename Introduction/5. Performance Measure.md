

## Root Mean Squared Error (RMSE)

The Root Mean Squared Error (RMSE) measures the average difference between values predicted by a model and the actual values.

### Formula:
$`RMSE(X, h) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2}`$

### Components:
- $`m`$ is the number of instances in the dataset
- $`x^{(i)}`$ is a vector of all feature values (excluding the label) of the $`i^{th}`$ instance in the dataset

  Example:

  $`x^{(1)} = \begin{bmatrix}
  -118.29 \\
  33.1 \\
  1.4234 \\
  48.98141
  \end{bmatrix}`$

  **Feature descriptions**:
  - $`-118.29`$: Longitude of the property.
  - $`33.1`$: Latitude of the property.
  - $`1.4234`$: Property size in thousands of square meters.
  - $`48.98141`$: Population density in the surrounding area.

  $`y^{(1)}`$ = 156.400

  where $`y^{(1)}`$ is the price of the house.

- $`X`$: A matrix of input features where each row corresponds to a single example, and each column represents a feature. Specifically:
  - In $`X`$, we have one row per instance.
  - The $`i^{th}`$ row of $`X`$ is equal to the transpose of $`x^{(i)}`$, noted $`(x^{(i)})^T`$.

Example:

$`X = \begin{bmatrix}
-118.29 & 33.1 & 1.4234 & 48.98141 \\
-117.85 & 34.1 & 2.3245 & 40.98321 \\
-119.10 & 32.5 & 1.9782 & 42.82100
\end{bmatrix}`$

- $`h`$ is our prediction function, also called $`{hypothesis}`$. So when our system is given an instance's feature vector $`x^{(i)}`$,
  it outputs a predicted value $`\hat{y}^{(i)}=h(x^{(i)})`$
- $`RMSE(X, h)`$ is the cost function measured on the set of examples using our hypothesis $`h`$

---

## Mean Absolute Error (MAE)

The **Mean Absolute Error (MAE)** is a metric used to evaluate the performance of regression models. It measures the average magnitude of the errors between the predicted and actual values, without considering their direction (i.e., it treats positive and negative errors equally).

### Formula:
$`MAE(X, h) = \frac{1}{m} \sum_{i=1}^{m} \left| h(x^{(i)}) - y^{(i)} \right|`$

Both $`RMSE`$ and the $`MAE`$ are ways to measure the distance between two vectors: the vector of predictions and the vector of target values.  

Various distance measures, or norms, are possible, here are two most basic examples:

- The **Euclidean norm** is the most commonly used distance metric. It measures the "straight-line" distance between two points in Euclidean space, akin to the Pythagorean theorem. It is essentially the same as the Root Mean Squared Error (RMSE) when applied to the differences between predicted and actual values in the context of regression tasks.
- The **Manhattan norm** measures the sum of the absolute differences between the components of two vectors.It measures the distance between two points in a city as if you can only travel along orthogonal city
  blocks. And it is the same as Mean Absolute Error (MAE).

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/990c44d2-7b90-427b-8c5d-18cde694a316">

---

## Cross Validation

Cross-validation is a technique used to test how well a machine learning model will perform on unseen data. 
It helps ensure the model is not just memorizing the training data (overfitting) but is generalizing to new data.

## Why Cross-Validation?

- A model might perform well on the training data but poorly on unseen data due to **overfitting**.
- Cross-validation gives a better estimate of how the model will perform in the real world by testing it on unseen parts of the data during training.

## Key Idea

Split the data into multiple parts, train the model on some parts, and test it on others. Repeat this process several times to get consistent and reliable results.


## K-Fold Cross-Validation (Most Common)

1. **Split the data into K parts (folds):**
    - If you have 100 data points, divide them into 5 groups (folds) of 20 points each.
    
2. **Train and test K times:**
    - In the first run, use 4 groups (80 points) for training and 1 group (20 points) for testing.
    - In the next run, use a different group for testing and the rest for training.
    - Repeat this until every group has been used for testing once.

3. **Calculate the average performance:**
    - At the end, average all the test results to estimate the model's performance.

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/f651381d-a07d-41fd-acc9-64500911fde8">

## Example of cross Validation in scikit-learn with 3 folds

```python
sgd_clf = SGDClassifier(random_state=42)
score = cross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring="accuracy")
```

---

## Confusion Matrix

A confusion matrix is a table used to evaluate the performance of a classification model. It compares the predicted labels with the actual labels and shows how well the model has classified each class. The general idea of a confusion matrix is to count the number of times instances of class A are classified as class B, for all A/B pairs. For example, to know the number of times the classifier confused the `spam` with `not_spam` we can look at the row 1 and column 2 and see that our algorithm classified `spam` as `not_spam` 300 times:

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/8b2e9293-89cb-433c-9453-2a4071be0780">

## Example of confusion matrix in scikit-learn

```python
sgd_clf = SGDClassifier()
sgd_clf.fit(x_train, y_train_5)
predictions = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3)
cm = confusion_matrix(y_train_5, predictions)
```

**A perfect classifier would look like this:**

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/643625f5-31c6-4896-8eb0-e6dc9ada2f9d">

---

## Precision and Recall

### Formal and more rigorous definition:

* **Precision** (Positive Predictive Value):
    * **Definition**: Precision measures the proportion of predicted positive cases (e.g., emails predicted as spam) that are actually positive (i.e., are truly spam).
    * **Formula**:
  $`\text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}`$
    * **High Precision** means that when the model says an email is spam, it's usually correct. However, it might miss some spam emails (false negatives).
* **Recall** (Sensitivity or True Positive Rate):
    * **Definition**: Recall measures the proportion of actual positive cases (e.g., spam emails) that the model successfully identifies as positive (i.e., correctly classifies as spam).
    * **Formula**: 
    $`\text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}`$
    * **High Recall** means the model catches most of the spam emails, but it might incorrectly flag some non-spam emails as spam (false positives).

### Example of difference between Recall and Precision in spam detection:

**Recall** answers the question: *Out of all the actual spam emails, how many did we correctly identify as spam?*

**Precision** answers the question: *Out of all the emails the model predicted as spam, how many were actually spam?*

### Example of usage of Recall and Precision in scikit-learn:

```python
from sklearn.metrics import precision_score, recall_score

precision = precision_score(y_values, predictions)
precision = recall_score(y_values, predictions)
```

It is often convenient to combine precision and recall into a single metric called the $`F_1`$ score, especially when you need a single metric to compare two classifiers. Fortunately scikit-learn provides a nice way to do it - the $`F_1`$ score is the *harmonic mean* of the precision and recall. Whereas the regular mean treats all values equally, the *harmonic mean* gives much more weight to low values. As a result, the classifier will only get a high score if both recall and precision are high.

```python
from sklearn.metrics import f1_score

score = f1_score(y_values, y_train_pred)
```

Unfortunately as I understand we can't max out both recall and precision, as increasing precision reduces the recall, and vice versa. Let's take a closer look from the example I'm implementing (following Chapter 3 which goal is to properly classify the handwritten digits, but in this example we do a binary classification and try to understand whether digit is 5 or not). I've been using the *SGDClassifier*, which for each instance computes a score based on a *decision function*. If that score is greater than a threshold, it assigns the instance to the positive class;otherwise it assigns it to the negative class. 

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/f9603585-d13e-4b36-bd3c-e27d095e2e1d">

To get a better intuition let's look at the picture which was shown as one of the examples in the book. As you can see there aree few digits and they are positioned from the lowest score on the left to the highest score on the right. Suppose the *decision threshold* is positioned at the central arrow: you fill find 4 true positives actual 5s and 1 false positive (actually a 6). Therefore with that threshold the precision is 80% (because out of all digits we predicted as 5s, 4 of them are actually 5s). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67%. If you raise the threshold (move it to the arrow on the right), the false positive (6) becomes a true negative, therefore our precision is increased to 100%, but one true positive becomes false negative, so the recall is decreased to 50% (as not out of 6 5s we predicted correctly only 3). Conversely, lowering the threshold increases recall and reduces precision.

One cool thing which is mentioned in the book is that we can play around with the threshold and even though seems like *SGDClassifier* doesn't allow us to specify the threshold, we can ask it to return the score of decision function and then based on that score we can define our own threshold. Here I'm leaving the example from [book resources](https://github.com/ageron/handson-ml3/blob/main/03_classification.ipynb) where we can visualize the relation between precision and recall. Pretty neat!

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/7169de80-6a0c-4b7b-832e-a4bb77f570f2">
