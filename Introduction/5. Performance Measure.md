

## Root Mean Squared Error (RMSE)

The Root Mean Squared Error (RMSE) measures the average difference between values predicted by a model and the actual values.

### Formula:
$`RMSE(X, h) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2}`$

### Components:
- $`m`$ is the number of instances in the dataset
- $`x^{(i)}`$ is a vector of all feature values (excluding the label) of the $`i^{th}`$ instance in the dataset

  Example:

  $`x^{(1)} = \begin{bmatrix}
  -118.29 \\
  33.1 \\
  1.4234 \\
  48.98141
  \end{bmatrix}`$

  **Feature descriptions**:
  - $`-118.29`$: Longitude of the property.
  - $`33.1`$: Latitude of the property.
  - $`1.4234`$: Property size in thousands of square meters.
  - $`48.98141`$: Population density in the surrounding area.

  $`y^{(1)}`$ = 156.400

  where $`y^{(1)}`$ is the price of the house.

- $`X`$: A matrix of input features where each row corresponds to a single example, and each column represents a feature. Specifically:
  - In $`X`$, we have one row per instance.
  - The $`i^{th}`$ row of $`X`$ is equal to the transpose of $`x^{(i)}`$, noted $`(x^{(i)})^T`$.

Example:

$`X = \begin{bmatrix}
-118.29 & 33.1 & 1.4234 & 48.98141 \\
-117.85 & 34.1 & 2.3245 & 40.98321 \\
-119.10 & 32.5 & 1.9782 & 42.82100
\end{bmatrix}`$

- $`h`$ is our prediction function, also called $`{hypothesis}`$. So when our system is given an instance's feature vector $`x^{(i)}`$,
  it outputs a predicted value $`\hat{y}^{(i)}=h(x^{(i)})`$
- $`RMSE(X, h)`$ is the cost function measured on the set of examples using our hypothesis $`h`$

---

## Mean Absolute Error (MAE)

The **Mean Absolute Error (MAE)** is a metric used to evaluate the performance of regression models. It measures the average magnitude of the errors between the predicted and actual values, without considering their direction (i.e., it treats positive and negative errors equally).

### Formula:
$`MAE(X, h) = \frac{1}{m} \sum_{i=1}^{m} \left| h(x^{(i)}) - y^{(i)} \right|`$

Both $`RMSE`$ and the $`MAE`$ are ways to measure the distance between two vectors: the vector of predictions and the vector of target values.  

Various distance measures, or norms, are possible, here are two most basic examples:

- The **Euclidean norm** is the most commonly used distance metric. It measures the "straight-line" distance between two points in Euclidean space, akin to the Pythagorean theorem. It is essentially the same as the Root Mean Squared Error (RMSE) when applied to the differences between predicted and actual values in the context of regression tasks.
- The **Manhattan norm** measures the sum of the absolute differences between the components of two vectors.It measures the distance between two points in a city as if you can only travel along orthogonal city
  blocks. And it is the same as Mean Absolute Error (MAE).

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/990c44d2-7b90-427b-8c5d-18cde694a316">

---

## Cross Validation

Cross-validation is a technique used to test how well a machine learning model will perform on unseen data. 
It helps ensure the model is not just memorizing the training data (overfitting) but is generalizing to new data.

## Why Cross-Validation?

- A model might perform well on the training data but poorly on unseen data due to **overfitting**.
- Cross-validation gives a better estimate of how the model will perform in the real world by testing it on unseen parts of the data during training.

## Key Idea

Split the data into multiple parts, train the model on some parts, and test it on others. Repeat this process several times to get consistent and reliable results.


## K-Fold Cross-Validation (Most Common)

1. **Split the data into K parts (folds):**
    - If you have 100 data points, divide them into 5 groups (folds) of 20 points each.
    
2. **Train and test K times:**
    - In the first run, use 4 groups (80 points) for training and 1 group (20 points) for testing.
    - In the next run, use a different group for testing and the rest for training.
    - Repeat this until every group has been used for testing once.

3. **Calculate the average performance:**
    - At the end, average all the test results to estimate the model's performance.

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/f651381d-a07d-41fd-acc9-64500911fde8">

## Example of cross Validation in scikit-learn with 3 folds

```python
sgd_clf = SGDClassifier(random_state=42)
score = cross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring="accuracy")
```

---

## Confusion Matrix

A confusion matrix is a table used to evaluate the performance of a classification model. It compares the predicted labels with the actual labels and shows how well the model has classified each class. The general idea of a confusion matrix is to count the number of times instances of class A are classified as class B, for all A/B pairs. For example, to know the number of times the classifier confused the `spam` with `not_spam` we can look at the row 1 and column 2 and see that our algorithm classified `spam` as `not_spam` 300 times:

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/8b2e9293-89cb-433c-9453-2a4071be0780">

## Example of confusion matrix in scikit-learn

```python
sgd_clf = SGDClassifier()
sgd_clf.fit(x_train, y_train_5)
predictions = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3)
cm = confusion_matrix(y_train_5, predictions)
```

**A perfect classifier would look like this:**

<img width="600" alt="Page 1" src="https://github.com/user-attachments/assets/643625f5-31c6-4896-8eb0-e6dc9ada2f9d">



