

## Root Mean Squared Error (RMSE)

The Root Mean Squared Error (RMSE) measures the average difference between values predicted by a model and the actual values.

### Formula:
$`RMSE(X, h) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2}`$

### Components:
- $`m`$ is the number of instances in the dataset
- $`x^{(i)}`$ is a vector of all feature values (excluding the label) of the $`i^{th}`$ instance in the dataset

  Example:

  $`x^{(1)} = \begin{bmatrix}
  -118.29 \\
  33.1 \\
  1.4234 \\
  48.98141
  \end{bmatrix}`$

  **Feature descriptions**:
  - $`-118.29`$: Longitude of the property.
  - $`33.1`$: Latitude of the property.
  - $`1.4234`$: Property size in thousands of square meters.
  - $`48.98141`$: Population density in the surrounding area.

  $`y^{(1)}`$ = 156.400

  where $`y^{(1)}`$ is the price of the house.

- $`X`$: A matrix of input features where each row corresponds to a single example, and each column represents a feature. Specifically:
  - In $`X`$, we have one row per instance.
  - The $`i^{th}`$ row of $`X`$ is equal to the transpose of $`x^{(i)}`$, noted $`(x^{(i)})^T`$.

Example:

$`X = \begin{bmatrix}
-118.29 & 33.1 & 1.4234 & 48.98141 \\
-117.85 & 34.1 & 2.3245 & 40.98321 \\
-119.10 & 32.5 & 1.9782 & 42.82100
\end{bmatrix}`$

- $`h`$ is our prediction function, also called $`{hypothesis}`$. So when our system is given an instance's feature vector $`x^{(i)}`$,
  it outputs a predicted value $`\hat{y}^{(i)}=h(x^{(i)})`$
- $`RMSE(X, h)`$ is the cost function measured on the set of examples using our hypothesis $`h`$

## Mean Absolute Error (MAE)

The **Mean Absolute Error (MAE)** is a metric used to evaluate the performance of regression models. It measures the average magnitude of the errors between the predicted and actual values, without considering their direction (i.e., it treats positive and negative errors equally).

### Formula:
$`MAE(X, h) = \frac{1}{m} \sum_{i=1}^{m} \left| h(x^{(i)}) - y^{(i)} \right|`$

---

Both $`RMSE`$ and the $`MAE`$ are ways to measure the distance between two vectors: the vector of predictions and the vector of target values.  

Various distance measures, or norms, are possible, here are two most basic examples:

- The **Euclidean norm** is the most commonly used distance metric. It measures the "straight-line" distance between two points in Euclidean space, akin to the Pythagorean theorem. It is essentially the same as the Root Mean Squared Error (RMSE) when applied to the differences between predicted and actual values in the context of regression tasks.
- The **Manhattan norm** measures the sum of the absolute differences between the components of two vectors.It measures the distance between two points in a city as if you can only travel along orthogonal city
  blocks. And it is the same as Mean Absolute Error (MAE).

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/990c44d2-7b90-427b-8c5d-18cde694a316">
