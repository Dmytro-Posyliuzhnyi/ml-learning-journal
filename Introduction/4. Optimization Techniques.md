
# Normal Equation

The **Normal Equation** is a mathematical formula used to calculate the optimal parameters (coefficients) for a linear regression model. It is a **closed-form solution**, meaning it provides the answer directly without the need for iterative methods like *gradient descent*. This makes it a simple and efficient method for solving linear regression problems when the dataset is relatively small.

### Formula
The Normal Equation is expressed as:

$`\boldsymbol{w} = \left( \boldsymbol{X}^\top \boldsymbol{X} \right)^{-1} \boldsymbol{X}^\top \boldsymbol{y}`$

Where:
- $`\boldsymbol{X}`$: The design matrix (includes the features and bias term),
- $`\boldsymbol{y}`$: The vector of target values,
- $`\boldsymbol{w}`$: The vector of model parameters (weights).


From what I understand, the *LinearRegression* class in *scikit-learn* uses *ordinary least squares* function, which is still a *closed-form* solution. From what my intuition tells me at the moment, it is essentially a more optimized form of the normal equation, which, for example, uses *Singular Value Decomposition* for the computation of the matrix pseudoinverse.

Also, I finally connected different concepts in my mind related to optimization terms like *MSE*, *RMSE*, *Least Squares*, etc. From what I understand, algorithms try to minimize the cost function, such as *MSE*. *Ordinary Least Squares (OLS)* is just one of the optimization techniques that minimizes the cost function, but it does so in a *closed-form* way. In contrast, *gradient descent* can also use MSE (which, from what I've read, is common for linear regression) but the difference is that it uses an iterative approach.

---

# Gradient Descent

**Gradient Descent** is an optimization technique used to minimize a function, typically the **loss function** in machine learning, by iteratively adjusting model parameters like weights $`w`$ and bias $`b`$. 
The goal is to find the values that reduce the error.


### Key Concepts
- **Gradient**: The gradient is a generalization of the slope to multiple dimensions. In one dimension, the slope is simply the derivative, but in higher dimensions,
  the gradient is a **vector** of partial derivatives with respect to each parameter. It tells you the direction of the steepest ascent.
  To minimize the function, we move in the opposite direction, steepest descent, that's why we have name **Gradient Descent**.

- **Learning Rate (Î±)**: A small positive value that determines the size of the steps taken during the gradient update. In more rigorous words: "The learning rate
  determines the magnitude of the changes to make to the weights and bias during each step of the gradient descent process."


## Epochs in Gradient Descent

An **epoch** refers to one complete pass through the entire training dataset. Each epoch consists of multiple steps where the model's parameters are updated 
based on the gradient of the loss function.

## Until Convergence

The phrase "until convergence" means the process continues until the model's parameters stop changing significantly. This happens when:
- The change in the loss function between epochs is small.
- The gradients become very close to zero.

At this point, the model has found the best parameters (or is very close to it), and further updates don't improve performance significantly.

## Types of Gradient Descent

### Batch Gradient Descent

Batch gradient descent, also called vanilla gradient descent, calculates the error for each instance within the training dataset, but it only gets updated (makes steps downhill) per epoch. Number of epochs is considered a hyperparameter. If it is too low, we will stop far away from the optimal solution; but if it is too high, we will waste time while the  model parameters do not change anymore. A simple solution is to set a very large number of epochs but to interrupt the algorithm when the gradient vector becomes tiny - that is, when its norm becomes smaller than a tiny number $`\epsilon`$ (called *tolerance*) - because this happens when gradient descent has (almost) reached the minimum. 

So as we can see, there are two important hyperparameters: $`\eta = \text{learning rate}`$ and $`\epsilon = \text{tolerance}`$.

**Convergence Rate**

When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), batch gradient descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take $`O(\frac{1}{\epsilon})`$ iterations to reach the optimum within a range of $`\epsilon`$, depending on the shape of the cost function. If you divide the tolerance by 10 to have a more precise solution, then the algorithm may have to run about 10 times longer.

### Stochastic Gradient Descent

The main problem with batch gradient descent is that is uses the whole training set to compute the gradients at every step, which makes it very slow when training set is large. At the opposite extreme **stochastic gradient descent** picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously, working on a single instance at a time make the algorithm much faster because it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.

On the other hand, dues to its stochastic (i.e., random) nature, this algorithm is much less regular than batch gradient descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to minimum, but once it get there it will continue to bounce around, never settling down. Once algorithm stops, the final parameter values will be good, but not optimal.

To perform linear regression using **stochastic gradient descent** with *scikit-learn*, you can use the *SGDRegressor* class, which defaults to optimizing the MSE cost function.

### Mini-Batch Gradient Descent

**Mini-Batch Gradient Descent** instead of computing the gradients on the full training set (as in *batch GD*) or based on just one instance (as in *stochastic GD*), **mini-batch GD** computes the gradients on small random sets of instances called *mini-batches*. The algorithm's progress in parameter space is less eratic than with *stochastic GD*, especially with fairly large *mini-batches*. As a result, **mini-batch GD** will end up walking around a bit closer to the minimum than *stochastic GD* - but it may be harder for it to escape from local minima. 

Here you can see the paths taken by the three gradient descent algorithms in parameter space during training. They all end up near the minimum, but *batch GD's* path actually stops at the minimum, while both *stochastic GD* and *mini-batch GD* continue to walk around.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/3f8716b3-2710-4bbb-9ab8-a1e635ebb358">

## Implementation

In [Gradient Descent.py](https://github.com/Dmytro-Posyliuzhnyi/ml-learning-journal/blob/main/Introduction/Code/Gradient%20Descent.py) I've included a basic implementation of gradient descent that tries to find the best model parameters to predict sales based on spending on radio advertising. I used Jupyter to execute it. The dataset was found on the wiki for the book "Hundred Page Machine Learning".

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/f66cfb4a-5ecd-4eab-8d1d-10df3765b239">

When using *gradient descent* we should ensure that all features have a similar scale, or else it will take much longer to converge.

---

**As summary in the book says, there is almost no difference after training: all these algorithms (Normal Equation, SVD, Batch GD, Stochastic GD, Mini-Batch GD) end up with very similar models and make predictions in exactly the same way.**
