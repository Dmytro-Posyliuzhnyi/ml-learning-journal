<details>
  <summary>Feature vector</summary>

### Feature vector

A **feature vector** is an ordered list of numerical values that represent the characteristics or properties (features) of an example in a dataset. Each value corresponds to a specific feature, and together, the vector provides a mathematical representation of the example that machine learning algorithms can process.

---

#### Example:

Imagine you are building a model to predict whether a person is likely to develop diabetes. Each person in your dataset is represented by a feature vector:

| Feature                  | Value   |
|--------------------------|---------|
| Age (in years)           | 45      |
| Body Mass Index (BMI)    | 28.5    |
| Glucose Level (mg/dL)    | 120     |
| Exercise Hours per Week  | 3       |

The feature vector for this individual would be:

$`x_i = [45, 28.5, 120, 3]`$

So basically the feature vector in our case is just 4-dimensional vector, which is treated as point in a high-dimensional space.

Feature vectors provide a standardized way to represent data points so that machine learning models can analyze and learn patterns from them.
</details>

<details>
  <summary>Hyperplane/Decision Boundary</summary>

### Hyperplane

*In linear classification algorithms the hyperplane is the same thing as decision boundary*

A **hyperplane** is a flat subspace in a higher-dimensional space that divides the space into two or more regions. In machine learning, hyperplanes sometimes are the same thing as decision boundaries, and decision boundary is used in algorithms to separate data points into different classes.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/717a5724-9631-4f6b-bfba-740429ed4b61">

In a **2D space**, a hyperplane is a **line**:
- $`2x_1 + 3x_2 - 5 = 0`$ represents a line dividing the plane into two regions.

In a **3D space**, a hyperplane is a **plane**:
- $`x_1 + 2x_2 + 3x_3 - 6 = 0`$ represents a plane splitting the 3D space.

In **higher dimensions**, it’s difficult to visualize, but the concept remains the same.

<details>
  <summary>Mathematical Definition</summary>


A hyperplane in a $`D`$-dimensional space is defined by the equation:

$`w_1x_1 + w_2x_2 + \dots + w_Dx_D + b = 0`$

Where:
- $`w_1, w_2, \dots, w_D`$ are the weights (coefficients) of the features.
- $`x_1, x_2, \dots, x_D`$ are the feature values of a data point.
- $`b`$ is the bias (intercept term).

Both weight and bias establish the hyperplane's orientation and position within the input space.

The hyperplane separates the space into regions based on the sign of the equation:
- $`w \cdot x + b > 0`$ on one side.
- $`w \cdot x + b < 0`$ on the other.

</details>

In non-linear models (e.g., Neural Networks, k-Nearest Neighbors) the decision boundary may not be a hyperplane - it could be a curved or irregular surface depending on the data and the model. For example a neural network might create a non-linear decision boundary that adapts to the data's complex shape.

Hyperplane is purely mathematical, while decision boundary is contextual:
- A hyperplane is always flat (linear) and mathematically defined.
- A decision boundary can be linear (a hyperplane) or non-linear, depending on the model.

---

</details>

<details>
  <summary>Margin</summary>

### Margin

The **margin** is the distance between the decision boundary (e.g., a hyperplane) and the closest data points from each class in a classification problem. It is a key concept in machine learning algorithms like **Support Vector Machines (SVMs)**.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/3846f0ec-b7d7-4a03-806f-1ea83462147f">

---

#### Why Is Margin Important?

1. **Generalization**:
   - A larger margin often leads to better generalization, meaning the model performs better on unseen data.

2. **Overfitting**:
   - A small margin increases the risk of overfitting, where the model becomes too sensitive to the training data.

3. **Robustness**:
   - Models with larger margins are less sensitive to small perturbations in the data.

---

</details>

<details>
  <summary>Optimization</summary>

**Optimization** is like the engine that makes machine learning work. At its core, it's all about finding the best values for a model's parameters (like weights and biases) so it performs well on a given task.

</details>

<details>
  <summary>Linear Models</summary>

### Linear Models

Linear models are one of the simplest types of machine learning algorithms. These models make predictions by finding a straight-line (or hyperplane in higher dimensions) relationship between the input features and the output.

---

#### Advantages of Linear Models:
- Easy to interpret (e.g., the coefficients show feature importance).
- Computationally efficient and fast to train.
- Works well when the relationship between features and the target is approximately linear.

#### Disadvantages of Linear Models:
- Struggles with non-linear relationships.
- Sensitive to outliers unless regularization techniques are used.

---

### When to Use Linear Models:
- When your data is linearly separable or has a roughly linear relationship.
- When you need a quick, interpretable model.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/f02e53a0-1c84-4640-97b0-3a369d9af74a">

</details>

<details>
  <summary>Kernels</summary>

### Kernels

Kernels are mathematical functions that enable machine learning algorithms, like Support Vector Machines (SVMs), to handle **non-linear data**. They work by implicitly mapping the original data into a higher-dimensional space where a linear decision boundary can be used.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/e0347f10-0552-4a22-81bc-438747522270">

---

#### Why Kernels Matter:
- They allow algorithms like SVMs to create non-linear decision boundaries.
- Kernels let you handle complex datasets without manually adding features or transforming data.

---

### When to Use Kernels:
- When your data is not linearly separable in the original feature space.
- When you suspect complex relationships between features but don’t want to explicitly define transformations.

</details>

<details>
  <summary>Parameters vs Hyperparameters</summary>
  <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/0335df3c-cbf7-44fe-8133-35154b988807">
</details>

<details>
<summary>Maximal Margin Classifier</summary>

The Maximal Margin Classifier is a machine learning method used to classify data by finding the hyperplane (or line in 2D) that separates two groups of points. It places the hyperplane **right in the middle** between the two closest points from each group (called support vectors) while maximizing the distance (margin) between the hyperplane and these points.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/1aa4f08a-a3b1-4909-a783-0c6189ba3032">

### Key Points:
- **Goal**: Place the hyperplane exactly in the middle of the support vectors to create the largest possible margin.
- **Works Best When**:
  - Data is perfectly separable.
  - There are no outliers or noise.
- **Limitations**:
  - It performs poorly when data is noisy or contains outliers, as these can shrink the margin and shift the hyperplane unfavorably.
    <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/48a59db0-799c-44b0-8eaf-014655b403da">

</details>


<details>
<summary>Bias/Variance Trade-off</summary>

The Bias/Variance trade-off is the balance between a model’s ability to generalize to unseen data and its ability to fit the training data:

- **Bias**: Error from oversimplified models that underfit the data (e.g., missing important patterns).
- **Variance**: Error from overly complex models that overfit the training data (e.g., capturing noise as patterns).

### Key Idea:
- High bias → Underfitting (poor performance on training and test data).
- High variance → Overfitting (good training performance but poor generalization).
- The goal is to find a balance for optimal performance on both.
  
</details>

<details>
<summary>Objective/Objective Function</summary>

In math, an objective function (or simply an objective) is the mathematical expression we want to either minimize or maximize during optimization.

For example:
- In economics, you might maximize profit.
- In engineering, you might minimize cost or error.
- In machine learning, you might minimize prediction error to make a model more accurate.
  
</details>

<details>
<summary>Loss Function and Cost Function</summary>
  
### **Loss Function**

A **loss function** is a mathematical function that measures the error between the predicted output of a machine learning model and the actual target value. It quantifies how "wrong" the model's prediction is for a **single data point**.

The loss function acts as a guide to help the model improve during training. By minimizing the loss, the model learns to make more accurate predictions.

---

### **Cost Function**

A **cost function**, on the other hand, is a mathematical function that measures the overall error of the model across the **entire dataset**. It aggregates the individual losses (calculated using the loss function) for all data points in the dataset into a single value. This value represents the model's overall performance.

---

### **Relationship Between Loss Function and Cost Function**

- The **loss function** calculates the error for a single data point.
- The **cost function** combines these errors for all data points, typically by summing or averaging them, to provide an overall measure of the model's performance.
- In many cases, the cost function is defined as the **average loss** over the dataset.

---

### **Example: Mean Squared Error (MSE)**

The **Mean Squared Error (MSE)** is a commonly used **cost function** in regression problems. It measures the average squared difference between the predicted values and the actual target values.

The MSE is calculated as:

$``\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (F_{w,b}(x_i) - y_i)^2``$

Where:
- **$`n`$**: The total number of data points in the dataset.
- **$`F_{w,b}(x_i)`$**: The predicted value for the $`i`$-th data point, generated by the model with parameters $`w`$ (weights) and $`b`$ (bias).
- **$`y_i`$**: The actual target value for the $`i`$-th data point.

### **Explanation**
1. **Sigma Notation ($``\sum``$)**:
   - The summation symbol ($``\sum_{i=1}^{n}``$) adds up the squared error $``(F_{w,b}(x_i) - y_i)^2``$ for all $`n`$ data points in the dataset.
   - This represents the **total error** across the dataset.

2. **Dividing by $`n`$**:
   - After summing the squared errors, dividing by $`n`$ gives the **average loss per data point**.
   - This ensures the result is normalized and independent of the dataset size, providing a more meaningful measure of error.

---

### **Key Takeaway**

- The **loss function** focuses on the error for a single data point.
- The **cost function** aggregates these errors across the dataset to provide an overall measure of the model's performance.
- Lower cost function values indicate better model performance. In regression, a lower MSE means the model's predictions are closer to the actual target values.

</details>


<details>
  <summary>Overfitting</summary>
    Overfitting happens in machine learning when a model learns the training data too well, including its noise and irrelevant details, instead of capturing the   
    general patterns. As a result, the model performs well on the training data but poorly on unseen (test or validation) data.
  <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/a03fa3ce-2778-4219-93c1-1a01d4446020">
</details>


<details>
  <summary>Gradient Descent</summary>
  <br/>
    Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving toward the function's lowest point. It is widely used in machine 
    learning, especially for training models by optimizing their parameters, such as in linear regression, logistic regression, and neural networks.

  <br/>
  <br/>
  
  ***The Goal***

  The goal of gradient descent is to find the minimum value of a function, often called the loss function or cost function, which measures how well a machine learning    model fits the data. For example:
  - In linear regression, the cost function is the mean squared error.
  - In classification, it could be the log-loss or cross-entropy loss.
  By minimizing the cost function, we improve the model's performance.
</details>

<details>
  <summary>Pruning</summary>
    <br/>
    <div>
      Pruning is like giving your machine learning model a much-needed haircut — removing unnecessary branches or parameters to make it leaner, faster, and more 
      accurate. Whether you're working with decision trees or deep neural networks, pruning can drastically enhance your model's performance while reducing its 
      complexity.
    <div/>
    <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/7e18fa0c-a24b-4329-ad22-dce374f3941c">
</details>

<details>
  <summary>Entropy</summary>
    <br/>
    <div>
      Entropy is a measure of uncertainty or impurity in a dataset. In the context of decision trees, it helps evaluate how mixed the data is at a node. 
      If all the data belongs to one class, the entropy is 0 (perfectly pure). If the data is evenly split between classes, the entropy is at its maximum 
      (most uncertain).
    <div/>
    <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/65357f18-e5bf-498d-ae3c-90bea7389c66">
</details>

<details>
  <summary>Normalization</summary>
    <br/>
    <div>
The goal of normalization is to scale the values of numeric columns in the dataset to a common range, typically between 0 and 1, without distorting the differences in the data or losing important information. For example, we may want to scale a numerical column with values ranging from -1000 to 5500 so that its new values fall between 0 and 1
    <div/>
</details>

<details>
  <summary>Standardization</summary>
    <br/>
    <div>
Standardization (also known as z-score normalization) is the process of scaling the features of a dataset so that they have a mean of 0 and a standard deviation of 1.
    <div/>
  <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/e5e78141-7c2e-40dd-ba16-53a8e8440a39">
</details>


<details>
  <summary>Regularization</summary>
    <br/>
    <div>
Regularization uses a range of techniques to correct for overfitting in machine learning models. As such, regularization is a method for increasing a model's generalizability - that is, it's ability to produce accurate predictions on new datasets.
<br/>
<br/>

**L1** regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator) regularization, introduces sparsity into the model feature coefficients.
This means it can set some feature coefficients to zero, effectively performing feature selection.
The mathematical basis of L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients.
The main advantage of L1 regularization is its ability to produce sparse models, reducing the complexity and making them easier to interpret.

<br/>

**L2** regularization, or Ridge, works differently than L1 by adding a penalty equal to the square of the magnitude of coefficients.
This type of regularization does not set coefficients to zero but rather reduces the impact of less important features.
The key difference from L1 is that all features remain part of the model, but their influence is balanced.
The squared terms in L2 encourage small, evenly distributed coefficient values, which helps improve model robustness.
    <div/>
</details>

<details>
  <summary>Softmax Function</summary>
    <br/>
    <div>
      
The **Softmax Function** is used to convert a vector of raw scores (logits) into probabilities that sum up to 1. It is commonly used in multi-class classification problems.

### Real-Life Example:
Imagine you're trying to identify the model of a car based on some features (e.g., color, size, brand). The model gives you raw scores (logits) for each possible car model, such as:

- Model A: 2.0
- Model B: 1.0
- Model C: 0.1

These are just raw scores, but you need to turn them into probabilities to understand which model is the most likely. 

The **Softmax function** converts these raw scores into probabilities, so you can interpret them as the chance of each model being the correct one. The sum of these probabilities will always equal 1.

For example, after applying softmax, you might get:

- Model A: 0.65 (65% chance it’s the right model)
- Model B: 0.25 (25% chance it’s the right model)
- Model C: 0.10 (10% chance it’s the right model)

### Summary:
- The **Softmax function** takes raw scores and converts them into probabilities.
- It's used in **multi-class classification** tasks, where you need to assign a probability to each possible class.
- The resulting probabilities always sum up to 1.
<div/>

| **Aspect**             | **Sigmoid**                                              | **Softmax**                                               |
|------------------------|----------------------------------------------------------|-----------------------------------------------------------|
| **Used For**           | Binary classification tasks (e.g., spam or not spam)     | Multi-class classification tasks (e.g., identifying a car model from multiple options) |
| **Output**             | Single probability (0 to 1)                              | Vector of probabilities (sum = 1)                         |
| **Range of Output**    | Between 0 and 1                                          | Between 0 and 1 for each class, but all probabilities sum to 1 |
| **Example**            | Predicting whether an email is spam or not               | Predicting the likelihood of an image belonging to one of multiple categories (e.g., cat, dog, or bird) |

</details>

<details>
<summary>One-Class Classification</summary>
<br/>

<div>
    
**One-Class Classification** also known as **unary classification** is a type of classification problem where the model is trained to recognize only a single class, often referred to as the "positive" class, while treating all other data as anomalies or outliers.
<div/>
  
**One-Class Classification** are used for outlier detection, anomaly detection, and novelty detection.

### Example:
- If you have a dataset of normal bank transactions, the model will learn the patterns of these transactions and flag anything that deviates significantly from the learned pattern as potential fraud.

</details>

<details>

<summary>Multi-Label Classification</summary>
<br/>
<div>
  
**Multi-Label Classification** is a type of classification problem where each instance (data point) can belong to multiple classes simultaneously, instead of just one class. For example, one image can be described with multiple labels, like: "car", "human", "road", etc.
<div/>
  
### How it works internally:
In multi-label classification, the model does not just assign a single label but rather outputs a set of labels. This is typically done by either:
1. **Binary Relevance**: Treating each label as an independent binary classification problem. For example, for a movie with three possible genres ("Action," "Adventure," and "Sci-Fi"), the model will independently decide whether the movie belongs to each genre or not. This results in three binary predictions: `Action = 1`, `Adventure = 1`, and `Sci-Fi = 0`.

2. **Classifier Chains**: A more advanced approach where classifiers are trained sequentially, with each classifier using the predictions of previous classifiers as additional input features. This way, the model can learn the relationships between labels, like how "Action" and "Adventure" genres often co-occur in movies.

3. **Label Powerset**: A method where all possible combinations of labels are treated as unique classes. This approach can be useful when the labels have complex relationships but can lead to a large number of combinations if there are many labels.


</details>
