<details>
  <summary>Feature vector</summary>

### Feature vector

A **feature vector** is an ordered list of numerical values that represent the characteristics or properties (features) of an example in a dataset. Each value corresponds to a specific feature, and together, the vector provides a mathematical representation of the example that machine learning algorithms can process.

---

#### Example:

Imagine you are building a model to predict whether a person is likely to develop diabetes. Each person in your dataset is represented by a feature vector:

| Feature                  | Value   |
|--------------------------|---------|
| Age (in years)           | 45      |
| Body Mass Index (BMI)    | 28.5    |
| Glucose Level (mg/dL)    | 120     |
| Exercise Hours per Week  | 3       |

The feature vector for this individual would be:

$`x_i = [45, 28.5, 120, 3]`$

So basically the feature vector in our case is just 4-dimensional vector, which is treated as point in a high-dimensional space.

Feature vectors provide a standardized way to represent data points so that machine learning models can analyze and learn patterns from them.
</details>

<details>
  <summary>Hyperplane/Decision Boundary</summary>

### Hyperplane

*In linear classification algorithms the hyperplane is the same thing as decision boundary*

A **hyperplane** is a flat subspace in a higher-dimensional space that divides the space into two or more regions. In machine learning, hyperplanes sometimes are the same thing as decision boundaries, and decision boundary is used in algorithms to separate data points into different classes.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/717a5724-9631-4f6b-bfba-740429ed4b61">

In a **2D space**, a hyperplane is a **line**:
- $`2x_1 + 3x_2 - 5 = 0`$ represents a line dividing the plane into two regions.

In a **3D space**, a hyperplane is a **plane**:
- $`x_1 + 2x_2 + 3x_3 - 6 = 0`$ represents a plane splitting the 3D space.

In **higher dimensions**, it’s difficult to visualize, but the concept remains the same.

<details>
  <summary>Mathematical Definition</summary>


A hyperplane in a $`D`$-dimensional space is defined by the equation:

$`w_1x_1 + w_2x_2 + \dots + w_Dx_D + b = 0`$

Where:
- $`w_1, w_2, \dots, w_D`$ are the weights (coefficients) of the features.
- $`x_1, x_2, \dots, x_D`$ are the feature values of a data point.
- $`b`$ is the bias (intercept term).

Both weight and bias establish the hyperplane's orientation and position within the input space.

The hyperplane separates the space into regions based on the sign of the equation:
- $`w \cdot x + b > 0`$ on one side.
- $`w \cdot x + b < 0`$ on the other.

</details>

In non-linear models (e.g., Neural Networks, k-Nearest Neighbors) the decision boundary may not be a hyperplane - it could be a curved or irregular surface depending on the data and the model. For example a neural network might create a non-linear decision boundary that adapts to the data's complex shape.

Hyperplane is purely mathematical, while decision boundary is contextual:
- A hyperplane is always flat (linear) and mathematically defined.
- A decision boundary can be linear (a hyperplane) or non-linear, depending on the model.

---

</details>

<details>
  <summary>Margin</summary>

### Margin

The **margin** is the distance between the decision boundary (e.g., a hyperplane) and the closest data points from each class in a classification problem. It is a key concept in machine learning algorithms like **Support Vector Machines (SVMs)**.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/3846f0ec-b7d7-4a03-806f-1ea83462147f">

---

#### Why Is Margin Important?

1. **Generalization**:
   - A larger margin often leads to better generalization, meaning the model performs better on unseen data.

2. **Overfitting**:
   - A small margin increases the risk of overfitting, where the model becomes too sensitive to the training data.

3. **Robustness**:
   - Models with larger margins are less sensitive to small perturbations in the data.

---

</details>

<details>
  <summary>Optimization</summary>

**Optimization** is like the engine that makes machine learning work. At its core, it's all about finding the best values for a model's parameters (like weights and biases) so it performs well on a given task.

</details>

<details>
  <summary>Linear Models</summary>

### Linear Models

Linear models are one of the simplest types of machine learning algorithms. These models make predictions by finding a straight-line (or hyperplane in higher dimensions) relationship between the input features and the output.

---

#### Advantages of Linear Models:
- Easy to interpret (e.g., the coefficients show feature importance).
- Computationally efficient and fast to train.
- Works well when the relationship between features and the target is approximately linear.

#### Disadvantages of Linear Models:
- Struggles with non-linear relationships.
- Sensitive to outliers unless regularization techniques are used.

---

### When to Use Linear Models:
- When your data is linearly separable or has a roughly linear relationship.
- When you need a quick, interpretable model.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/f02e53a0-1c84-4640-97b0-3a369d9af74a">

</details>

<details>
  <summary>Kernels</summary>

### Kernels

Kernels are mathematical functions that enable machine learning algorithms, like Support Vector Machines (SVMs), to handle **non-linear data**. They work by implicitly mapping the original data into a higher-dimensional space where a linear decision boundary can be used.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/e0347f10-0552-4a22-81bc-438747522270">

---

#### Why Kernels Matter:
- They allow algorithms like SVMs to create non-linear decision boundaries.
- Kernels let you handle complex datasets without manually adding features or transforming data.

---

### When to Use Kernels:
- When your data is not linearly separable in the original feature space.
- When you suspect complex relationships between features but don’t want to explicitly define transformations.

</details>

<details>
  <summary>Parameters vs Hyperparameters</summary>
  <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/0335df3c-cbf7-44fe-8133-35154b988807">
</details>

<details>
<summary>Maximal Margin Classifier</summary>

The Maximal Margin Classifier is a machine learning method used to classify data by finding the hyperplane (or line in 2D) that separates two groups of points. It places the hyperplane **right in the middle** between the two closest points from each group (called support vectors) while maximizing the distance (margin) between the hyperplane and these points.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/1aa4f08a-a3b1-4909-a783-0c6189ba3032">

### Key Points:
- **Goal**: Place the hyperplane exactly in the middle of the support vectors to create the largest possible margin.
- **Works Best When**:
  - Data is perfectly separable.
  - There are no outliers or noise.
- **Limitations**:
  - It performs poorly when data is noisy or contains outliers, as these can shrink the margin and shift the hyperplane unfavorably.
    <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/48a59db0-799c-44b0-8eaf-014655b403da">

</details>


<details>
<summary>Bias/Variance Trade-off</summary>

The Bias/Variance trade-off is the balance between a model’s ability to generalize to unseen data and its ability to fit the training data:

- **Bias**: Error from oversimplified models that underfit the data (e.g., missing important patterns).
- **Variance**: Error from overly complex models that overfit the training data (e.g., capturing noise as patterns).

### Key Idea:
- High bias → Underfitting (poor performance on training and test data).
- High variance → Overfitting (good training performance but poor generalization).
- The goal is to find a balance for optimal performance on both.
  
</details>

<details>
<summary>Objective/Objective Function</summary>

In math, an objective function (or simply an objective) is the mathematical expression we want to either minimize or maximize during optimization.

For example:
- In economics, you might maximize profit.
- In engineering, you might minimize cost or error.
- In machine learning, you might minimize prediction error to make a model more accurate.
  
</details>

<details>
<summary>Loss Function and Cost Function</summary>
  
### **Loss Function**

A **loss function** is a mathematical function that measures the error between the predicted output of a machine learning model and the actual target value. It quantifies how "wrong" the model's prediction is for a **single data point**.

The loss function acts as a guide to help the model improve during training. By minimizing the loss, the model learns to make more accurate predictions.

---

### **Cost Function**

A **cost function**, on the other hand, is a mathematical function that measures the overall error of the model across the **entire dataset**. It aggregates the individual losses (calculated using the loss function) for all data points in the dataset into a single value. This value represents the model's overall performance.

---

### **Relationship Between Loss Function and Cost Function**

- The **loss function** calculates the error for a single data point.
- The **cost function** combines these errors for all data points, typically by summing or averaging them, to provide an overall measure of the model's performance.
- In many cases, the cost function is defined as the **average loss** over the dataset.

---

### **Example: Mean Squared Error (MSE)**

The **Mean Squared Error (MSE)** is a commonly used **cost function** in regression problems. It measures the average squared difference between the predicted values and the actual target values.

The MSE is calculated as:

$``\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (F_{w,b}(x_i) - y_i)^2``$

Where:
- **$`n`$**: The total number of data points in the dataset.
- **$`F_{w,b}(x_i)`$**: The predicted value for the $`i`$-th data point, generated by the model with parameters $`w`$ (weights) and $`b`$ (bias).
- **$`y_i`$**: The actual target value for the $`i`$-th data point.

### **Explanation**
1. **Sigma Notation ($``\sum``$)**:
   - The summation symbol ($``\sum_{i=1}^{n}``$) adds up the squared error $``(F_{w,b}(x_i) - y_i)^2``$ for all $`n`$ data points in the dataset.
   - This represents the **total error** across the dataset.

2. **Dividing by $`n`$**:
   - After summing the squared errors, dividing by $`n`$ gives the **average loss per data point**.
   - This ensures the result is normalized and independent of the dataset size, providing a more meaningful measure of error.

---

### **Key Takeaway**

- The **loss function** focuses on the error for a single data point.
- The **cost function** aggregates these errors across the dataset to provide an overall measure of the model's performance.
- Lower cost function values indicate better model performance. In regression, a lower MSE means the model's predictions are closer to the actual target values.

</details>


<details>
  <summary>Overfitting</summary>
    Overfitting happens in machine learning when a model learns the training data too well, including its noise and irrelevant details, instead of capturing the   
    general patterns. As a result, the model performs well on the training data but poorly on unseen (test or validation) data.
  <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/a03fa3ce-2778-4219-93c1-1a01d4446020">
</details>


<details>
  <summary>Gradient Descent</summary>
  <br/>
    Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving toward the function's lowest point. It is widely used in machine 
    learning, especially for training models by optimizing their parameters, such as in linear regression, logistic regression, and neural networks.

  <br/>
  <br/>
  
  ***The Goal***

  The goal of gradient descent is to find the minimum value of a function, often called the loss function or cost function, which measures how well a machine learning    model fits the data. For example:
  - In linear regression, the cost function is the mean squared error.
  - In classification, it could be the log-loss or cross-entropy loss.
  By minimizing the cost function, we improve the model's performance.
</details>

<details>
  <summary>Pruning</summary>
    <br/>
    <div>
      Pruning is like giving your machine learning model a much-needed haircut — removing unnecessary branches or parameters to make it leaner, faster, and more 
      accurate. Whether you're working with decision trees or deep neural networks, pruning can drastically enhance your model's performance while reducing its 
      complexity.
    <div/>
    <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/7e18fa0c-a24b-4329-ad22-dce374f3941c">
</details>

<details>
  <summary>Entropy</summary>
    <br/>
    <div>
      Entropy is a measure of uncertainty or impurity in a dataset. In the context of decision trees, it helps evaluate how mixed the data is at a node. 
      If all the data belongs to one class, the entropy is 0 (perfectly pure). If the data is evenly split between classes, the entropy is at its maximum 
      (most uncertain).
    <div/>
    <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/65357f18-e5bf-498d-ae3c-90bea7389c66">
</details>

<details>
  <summary>Normalization</summary>
    <br/>
    <div>
The goal of normalization is to scale the values of numeric columns in the dataset to a common range, typically between 0 and 1, without distorting the differences in the data or losing important information. For example, we may want to scale a numerical column with values ranging from -1000 to 5500 so that its new values fall between 0 and 1
    <div/>
</details>

<details>
  <summary>Standardization</summary>
    <br/>
    <div>
Standardization (also known as z-score normalization) is the process of scaling the features of a dataset so that they have a mean of 0 and a standard deviation of 1.
    <div/>
  <img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/e5e78141-7c2e-40dd-ba16-53a8e8440a39">
</details>


<details>
  <summary>Regularization</summary>
    <br/>
    <div>
Regularization uses a range of techniques to correct for overfitting in machine learning models. As such, regularization is a method for increasing a model's generalizability - that is, it's ability to produce accurate predictions on new datasets.
<br/>
<br/>

**L1** regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator) regularization, introduces sparsity into the model feature coefficients.
This means it can set some feature coefficients to zero, effectively performing feature selection.
The mathematical basis of L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients.
The main advantage of L1 regularization is its ability to produce sparse models, reducing the complexity and making them easier to interpret.

<br/>

**L2** regularization, or Ridge, works differently than L1 by adding a penalty equal to the square of the magnitude of coefficients.
This type of regularization does not set coefficients to zero but rather reduces the impact of less important features.
The key difference from L1 is that all features remain part of the model, but their influence is balanced.
The squared terms in L2 encourage small, evenly distributed coefficient values, which helps improve model robustness.
    <div/>
</details>

<details>
  <summary>Softmax Function</summary>
    <br/>
    <div>
      
The **Softmax Function** is used to convert a vector of raw scores (logits) into probabilities that sum up to 1. It is commonly used in multi-class classification problems.

### Real-Life Example:
Imagine you're trying to identify the model of a car based on some features (e.g., color, size, brand). The model gives you raw scores (logits) for each possible car model, such as:

- Model A: 2.0
- Model B: 1.0
- Model C: 0.1

These are just raw scores, but you need to turn them into probabilities to understand which model is the most likely. 

The **Softmax function** converts these raw scores into probabilities, so you can interpret them as the chance of each model being the correct one. The sum of these probabilities will always equal 1.

For example, after applying softmax, you might get:

- Model A: 0.65 (65% chance it’s the right model)
- Model B: 0.25 (25% chance it’s the right model)
- Model C: 0.10 (10% chance it’s the right model)

### Summary:
- The **Softmax function** takes raw scores and converts them into probabilities.
- It's used in **multi-class classification** tasks, where you need to assign a probability to each possible class.
- The resulting probabilities always sum up to 1.
<div/>

| **Aspect**             | **Sigmoid**                                              | **Softmax**                                               |
|------------------------|----------------------------------------------------------|-----------------------------------------------------------|
| **Used For**           | Binary classification tasks (e.g., spam or not spam)     | Multi-class classification tasks (e.g., identifying a car model from multiple options) |
| **Output**             | Single probability (0 to 1)                              | Vector of probabilities (sum = 1)                         |
| **Range of Output**    | Between 0 and 1                                          | Between 0 and 1 for each class, but all probabilities sum to 1 |
| **Example**            | Predicting whether an email is spam or not               | Predicting the likelihood of an image belonging to one of multiple categories (e.g., cat, dog, or bird) |

</details>

<details>
<summary>One-Class Classification</summary>
<br/>

<div>
    
**One-Class Classification** also known as **unary classification** is a type of classification problem where the model is trained to recognize only a single class, often referred to as the "positive" class, while treating all other data as anomalies or outliers.
<div/>
  
**One-Class Classification** are used for outlier detection, anomaly detection, and novelty detection.

### Example:
- If you have a dataset of normal bank transactions, the model will learn the patterns of these transactions and flag anything that deviates significantly from the learned pattern as potential fraud.

</details>

<details>

<summary>Multi-Label Classification</summary>
<br/>
<div>
  
**Multi-Label Classification** is a type of classification problem where each instance (data point) can belong to multiple classes simultaneously, instead of just one class. For example, one image can be described with multiple labels, like: "car", "human", "road", etc.
<div/>
  
### How it works internally:
In multi-label classification, the model does not just assign a single label but rather outputs a set of labels. This is typically done by either:
1. **Binary Relevance**: Treating each label as an independent binary classification problem. For example, for a movie with three possible genres ("Action," "Adventure," and "Sci-Fi"), the model will independently decide whether the movie belongs to each genre or not. This results in three binary predictions: `Action = 1`, `Adventure = 1`, and `Sci-Fi = 0`.

2. **Classifier Chains**: A more advanced approach where classifiers are trained sequentially, with each classifier using the predictions of previous classifiers as additional input features. This way, the model can learn the relationships between labels, like how "Action" and "Adventure" genres often co-occur in movies.

3. **Label Powerset**: A method where all possible combinations of labels are treated as unique classes. This approach can be useful when the labels have complex relationships but can lead to a large number of combinations if there are many labels.


</details>

<details>
<summary>Ensemble Learning</summary>

<br/>

**Ensemble Learning** is a technique in machine learning where multiple models (often referred to as "learners") are trained and combined to solve the same problem. It's a learning paradigm that, instead of trying to learn one super-acurate model, focuses on training a large number of low-accuracy models and then combining the predictions gives by those weal models to obtain a high-accuracy **meta-model**.

### Key Approaches:

1. **Boosting**:
   - **Concept**: Train models sequentially, where each new model focuses on correcting the errors made by the previous one.
   - **Goal**: Reduce bias and improve prediction accuracy by building a strong model from a series of weak learners.
   - **Analogy**: Picture a team of students solving a complex puzzle together. Each student tackles mistakes left by others, improving the solution step by step.

2. **Bagging (Bootstrap Aggregating)**:
   - **Concept**: Train multiple models independently on different subsets of the data and combine their outputs (e.g., averaging for regression or voting for classification). 
   - **Goal**: Reduce variance and improve model stability.
   - **Example**: Random Forest.
   - **Analogy**: Imagine 10 meteorologists predicting tomorrow's weather based on slightly different datasets. You trust the average of their predictions rather than relying on just one.

</details>

<details>
<summary>Sequence Labeling</summary>

<br/>

**Sequence Labeling** is a supervised learning task in machine learning where the goal is to assign a label to each element in a sequence of inputs. It is commonly used for tasks involving structured data, where the order or context of elements in the sequence is important.

### Key Characteristics:
- **Input**: A sequence of elements (e.g., words in a sentence, DNA base pairs, audio signals).
- **Output**: A sequence of labels, with one label for each input element.

### Examples of Applications:
1. **Natural Language Processing (NLP)**:
   - **Part-of-Speech Tagging**: Label each word in a sentence with its grammatical role (e.g., noun, verb).
   - **Named Entity Recognition (NER)**: Identify entities like names, dates, or locations in text.
   - **Chunking**: Group words into meaningful chunks, like noun phrases.

2. **Speech Processing**:
   - Assign phonemes to segments of audio data.

3. **Bioinformatics**:
   - Classify DNA or protein sequences into functional regions.

4. **Time Series Analysis**:
   - Label segments of sensor data (e.g., detecting anomalies in IoT data).

</details>

<details>
<summary>Sequence-to-Sequence Learning</summary>

<br/>

**Sequence-to-Sequence (Seq2Seq) Learning** is a machine learning paradigm designed to transform an input sequence into an output sequence, where both sequences can have different lengths. This approach is commonly used in tasks where the relationship between input and output sequences requires context-dependent transformations.

### Key Characteristics:
- **Input**: A sequence of elements (e.g., words in a sentence, audio signals).
- **Output**: Another sequence of elements (e.g., translated text, summaries).
- Both sequences may vary in length and structure.

### How It Works:
1. **Encoder**: Processes the input sequence and compresses its information into a fixed-size representation (a context vector or hidden states).
2. **Decoder**: Takes this representation and generates the output sequence step-by-step.
3. Often uses attention mechanisms to focus on relevant parts of the input sequence while decoding.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/6c782cac-b388-45a5-a95d-1a8e2a4cf417">

### Applications:
1. **Natural Language Processing (NLP)**:
   - **Machine Translation**: Translating text from one language to another (e.g., English to French).
   - **Text Summarization**: Condensing long texts into shorter summaries.
   - **Speech Recognition**: Converting audio into text.

2. **Computer Vision**:
   - **Image Captioning**: Generating textual descriptions of images.

3. **Bioinformatics**:
   - Predicting DNA sequences or converting genomic data into meaningful outputs.

</details>

<details>
<summary>Active Learning</summary>

<br/>

**Active Learning** is a machine learning approach that focuses on improving model performance by strategically selecting the most informative data points for labeling. Instead of randomly labeling data, the model "asks" for labels on the data points it is most uncertain about, minimizing labeling effort while maximizing learning efficiency.

### Key Concepts
1. **Why Use Active Learning?**
   - Labeling data is expensive and time-consuming.
   - Not all data points contribute equally to improving model performance.
   - By selecting the most "uncertain" or "informative" samples, active learning reduces the amount of labeled data required.
2. **How Does It Work?**
   - The model is trained on an initial labeled dataset.
   - It evaluates the unlabeled dataset and identifies data points where it has the least confidence or expects the most disagreement.
   - These selected samples are sent to an oracle (e.g., a human expert) for labeling.
   - The newly labeled data points are added to the training set, and the process repeats.
3. **Query Strategies**:
   - **Uncertainty Sampling**: Choose the samples the model is least confident about (e.g., closest to decision boundaries).
   - **Query-by-Committee**: Use multiple models to identify samples with the most disagreement among predictions.
   - **Diversity Sampling**: Select a diverse set of data points to improve generalization.

### Real-Life Analogy
Imagine teaching a student math. Instead of explaining every problem, you focus on the ones they find most confusing. By addressing these specific challenges, the student learns faster and with less effort.
  
</details>


<details>
<summary>Association Rule Learning</summary>

<br/>

**Association rule learning** is a rule-based machine learning method for discovering interesting relations between variables in large databases.

For example in a retail setting, association rule learning is often used to discover relationships between items that are frequently bought together. For example, algorithms may notice that when users buy item A, they frequently combine it with item B. This insight can help you place these items together in the store or recommend them close to each other in an online store.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/87795897-e22c-4d6f-860f-12c4b9ca2bce">

</details>
