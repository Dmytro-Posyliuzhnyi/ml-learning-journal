
# Linear Regression

Linear regression is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. 
The variable you are using to predict the other variable's value is called the independent variable. It works by finding the best-fit line (or hyperplane for multiple variables) 
that shows the relationship between these variables.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/63b4f083-192e-40fb-a45c-71b26210cca6">

For simple linear regression (one feature) the equation look in the following way: $`y = wx + b`$, where:
  - $`y`$: Predicted value (output)
  - $`x`$: Input feature (independent variable)
  - $`w`$: Weight (slope of the line)
  - $`b`$: Bias (intercept, where the line crosses the y-axis)

The primary goal in training a linear regression model is to minimize the cost function.

**How It Works**


1. **Fit the Model**:
    - Use historical data to calculate the parameters ($`w`$ and $`b`$) that minimize the cost function.

2. **Make Predictions**:
    - Plug new input values ($`x`$) into the regression equation to predict $`y`$.

3. **Example**:
    Let's take a look at trivial example which can give us some intuition being the linear regression. Suppose we want to predict a student's exam score ($`y`$)
    based on hours studied ($`x`$).

    Data:
    | Hours Studied ($`x`$) | Exam Score ($`y`$) |
    |------------------------|-------------------|
    | 2                      | 50                |
    | 4                      | 60                |
    | 6                      | 70                |
    | 8                      | 80                |

    Model:
    $`y = 5x + 40`$

    Prediction for 7 hours studied:
    $`y = 5(7) + 40 = 75`$

You can think of Linear Regression as a function that finds the best-fit line through historical data, allowing you to predict, with high accuracy, 
values for points that were not part of the training data. Essentially, it fills in the gaps based on the existing data points. 
For example, if I give you a Cartesian plane with 10 points plotted on it and tell you that these points are the result of some underlying function, 
Linear Regression can draw a line that closely approximates the original function. This ensures that the $`y`$-values of the line are not too far from the 
original function's values.

---

# Logistic Regression

Logistic Regression is a **classification algorithm** widely used for binary classification tasks (e.g., Spam/Not Spam, Yes/No, Dog/No Dog). Despite its name, it is not a regression algorithm for predicting continuous values. Instead, it predicts the **probability** of a class, which is then thresholded to classify the data.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/8d83cfdd-d2cb-4162-b616-b5dc86ad73a5">

Logistic regression works by:
1. Computing a **linear equation**:
   $`z = wx + b`$,
   where:
   - $`w`$: Weight (influence of features),
   - $`b`$: Bias (adjustment to the threshold),
   - $`z`$: Linear score (not yet a probability).
2. Passing $`z`$ through the **sigmoid function** to output probabilities:
   $`f(z) = \frac{1}{1 + e^{-z}}`$.
3. Classifying based on a threshold (default: 0.5):
   - $`P(y=1 | x) \geq 0.5`$: Predict class 1.
   - $`P(y=1 | x) < 0.5`$: Predict class 0.

<details>
  
<summary>What is the Sigmoid Function?</summary>
  
The **sigmoid function** is a mathematical function used to "squash" any real number into a range between 0 and 1. This makes it perfect for predicting probabilities.

#### **Sigmoid Formula**:

$`f(z) = \frac{1}{1 + e^{-z}}`$

</details>

**How Does Logistic Regression Work?**

1. **Training**:
   - Logistic Regression learns the best $`w`$ and $`b`$ by maximizing the **likelihood function**:
     $`L(w, b) = \prod_{i=1}^n P(y_i | x_i)`$.
   - In practice, the **log-likelihood** is used for numerical stability:
     $`\log L(w, b) = \sum_{i=1}^n \big( y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \big)`$.

2. **Prediction**:
   - For a data point $`x`$, compute $`z = wx + b`$.
   - Pass $`z`$ through the sigmoid function to get $`\hat{y}`$, the probability of the positive class.
   - Classify based on the threshold (e.g., 0.5).

---

# Decision Tree Learning


### Overview
The Decision Tree is a **supervised machine learning algorithm**. It works by splitting the dataset into subsets based on feature conditions, organizing these splits into a tree-like structure.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/6ec8ce57-0d18-4cd0-a8b8-bc77c0d4eb72">

### Key Concepts

#### **Nodes and Splits**
- **Root Node:** The starting point, where the first decision is made.
- **Internal Nodes:** Decision points that split the data further.
- **Leaf Nodes:** Final nodes that contain the prediction (class label or value).

#### **Questions in the Tree**
Each question corresponds to a condition on a feature (e.g., "Is Math Grade > 75?"). These questions divide the dataset into groups. The algorithm evaluates all possible splits and selects the one that optimizes a specific metric.

---

### Metrics for Optimization

#### **For Classification Trees:**

1. **Entropy** (measures impurity):
   - High entropy means mixed classes; low entropy means pure classes.

2. **Gini Impurity:**
   - Measures the likelihood of misclassification.

3. **Information Gain (IG):**
   - Measures how much splitting reduces entropy:

---

### Real-Life Example (Classification)

#### Predicting Exam Results
| Student | Math Grade | Hours Studied | Passed Exam |
|---------|------------|---------------|-------------|
| Alice   | 85         | 10            | Yes         |
| Bob     | 65         | 5             | No          |
| Charlie | 90         | 12            | Yes         |
| Diana   | 50         | 3             | No          |

**Tree Structure:**
1. Split: "Is Math Grade > 75?"
   - **Yes:** Likely to pass.
   - **No:** Ask another question.
2. Next split (for "No"): "Did Hours Studied > 5?"
   - **Yes:** Likely to pass.
   - **No:** Likely to fail.

---

### Analogy: Navigating a Maze
Imagine navigating a maze to reach the exit:
- At each intersection, you choose a direction ("Left or Right?") based on visible signs or rules.
- Each decision narrows the paths available, guiding you closer to the exit.

In the tree:
- **Entropy:** Represents the confusion about where the exit might be (a more complex maze has higher entropy).
- **Splits:** Each decision reduces confusion and eliminates wrong paths.
- **Pruning:** Removing unnecessary paths from the maze, simplifying the solution without changing the outcome.

---

