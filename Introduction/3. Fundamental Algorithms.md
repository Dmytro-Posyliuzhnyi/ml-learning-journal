
# Linear Regression

Linear regression is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. 
The variable you are using to predict the other variable's value is called the independent variable. It works by finding the best-fit line (or hyperplane for multiple variables) 
that shows the relationship between these variables.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/63b4f083-192e-40fb-a45c-71b26210cca6">

For simple linear regression (one feature) the equation look in the following way: $`y = wx + b`$, where:
  - $`y`$: Predicted value (output)
  - $`x`$: Input feature (independent variable)
  - $`w`$: Weight (slope of the line)
  - $`b`$: Bias (intercept, where the line crosses the y-axis)

The primary goal in training a linear regression model is to minimize the cost function.

**How It Works**


1. **Fit the Model**:
    - Use historical data to calculate the parameters ($`w`$ and $`b`$) that minimize the cost function.

2. **Make Predictions**:
    - Plug new input values ($`x`$) into the regression equation to predict $`y`$.

3. **Example**:
    Let's take a look at trivial example which can give us some intuition being the linear regression. Suppose we want to predict a student's exam score ($`y`$)
    based on hours studied ($`x`$).

    Data:
    | Hours Studied ($`x`$) | Exam Score ($`y`$) |
    |------------------------|-------------------|
    | 2                      | 50                |
    | 4                      | 60                |
    | 6                      | 70                |
    | 8                      | 80                |

    Model:
    $`y = 5x + 40`$

    Prediction for 7 hours studied:
    $`y = 5(7) + 40 = 75`$

You can think of Linear Regression as a function that finds the best-fit line through historical data, allowing you to predict, with high accuracy, 
values for points that were not part of the training data. Essentially, it fills in the gaps based on the existing data points. 
For example, if I give you a Cartesian plane with 10 points plotted on it and tell you that these points are the result of some underlying function, 
Linear Regression can draw a line that closely approximates the original function. This ensures that the $`y`$-values of the line are not too far from the 
original function's values.

---

# Logistic Regression

Logistic Regression is a **classification algorithm** widely used for binary classification tasks (e.g., Spam/Not Spam, Yes/No, Dog/No Dog). Despite its name, it is not a regression algorithm for predicting continuous values. Instead, it predicts the **probability** of a class, which is then thresholded to classify the data.

<img width="500" alt="Page 1" src="https://github.com/user-attachments/assets/8d83cfdd-d2cb-4162-b616-b5dc86ad73a5">

Logistic regression works by:
1. Computing a **linear equation**:
   $`z = wx + b`$,
   where:
   - $`w`$: Weight (influence of features),
   - $`b`$: Bias (adjustment to the threshold),
   - $`z`$: Linear score (not yet a probability).
2. Passing $`z`$ through the **sigmoid function** to output probabilities:
   $`f(z) = \frac{1}{1 + e^{-z}}`$.
3. Classifying based on a threshold (default: 0.5):
   - $`P(y=1 | x) \geq 0.5`$: Predict class 1.
   - $`P(y=1 | x) < 0.5`$: Predict class 0.

<details>
  
<summary>What is the Sigmoid Function?</summary>
  
The **sigmoid function** is a mathematical function used to "squash" any real number into a range between 0 and 1. This makes it perfect for predicting probabilities.

#### **Sigmoid Formula**:

$`f(z) = \frac{1}{1 + e^{-z}}`$

</details>

**How Does Logistic Regression Work?**

1. **Training**:
   - Logistic Regression learns the best $`w`$ and $`b`$ by maximizing the **likelihood function**:
     $`L(w, b) = \prod_{i=1}^n P(y_i | x_i)`$.
   - In practice, the **log-likelihood** is used for numerical stability:
     $`\log L(w, b) = \sum_{i=1}^n \big( y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \big)`$.

2. **Prediction**:
   - For a data point $`x`$, compute $`z = wx + b`$.
   - Pass $`z`$ through the sigmoid function to get $`\hat{y}`$, the probability of the positive class.
   - Classify based on the threshold (e.g., 0.5).

---

