# Cross Validation

Cross-validation is a technique used to test how well a machine learning model will perform on unseen data. 
It helps ensure the model is not just memorizing the training data (overfitting) but is generalizing to new data.

## Why Cross-Validation?

- A model might perform well on the training data but poorly on unseen data due to **overfitting**.
- Cross-validation gives a better estimate of how the model will perform in the real world by testing it on unseen parts of the data during training.


## Key Idea

Split the data into multiple parts, train the model on some parts, and test it on others. Repeat this process several times to get consistent and reliable results.


## K-Fold Cross-Validation (Most Common)

1. **Split the data into K parts (folds):**
    - If you have 100 data points, divide them into 5 groups (folds) of 20 points each.
    
2. **Train and test K times:**
    - In the first run, use 4 groups (80 points) for training and 1 group (20 points) for testing.
    - In the next run, use a different group for testing and the rest for training.
    - Repeat this until every group has been used for testing once.

3. **Calculate the average performance:**
    - At the end, average all the test results to estimate the model's performance.

<img width="900" alt="Page 1" src="https://github.com/user-attachments/assets/f651381d-a07d-41fd-acc9-64500911fde8">

## Example of cross Validation in scikit-learn with 3 folds

```python
sgd_clf = SGDClassifier(random_state=42)
score = cross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring="accuracy")
```
