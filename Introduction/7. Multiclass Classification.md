
## One-versus-the-rest strategy

Some scikit-learn classifiers are capable of handling multiple classes natively, while others are strictly binary classifiers. However, there are various strategies that can be used to perform multiclass classification with multiple binary classifiers. Let's use an example from the book I'm currently reading, where we're trying to create a model that will classify images of digits from 0 to 9. One way to use binary classifiers in multiclass classification, in our case, would be to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, etc.). Then, when you want to classify an image, you get the decision score from each classifier for that image and select the class whose classifier outputs the highest score. This is called the **one-versus-the-rest** strategy or **one-versus-all**.

## One-versus-one strategy

Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the **one-versus-one** strategy. If there are *N* classes, you need to train $`N \times (N - 1) / 2`$. For the *MNIST* problem, this means you need to train 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advantage of **OvO** is that each classifier only needs to be trained on the part of the training set containing the two classes that it must distinguish.

Some algorithms scale poorly with the size of the training set. For these algorithms, **OvO** is preferred because it is faster to train many classifiers on small training sets than to train few classifiers on large training sets. For most binary classification algorithms, however, **OvR** is preferred.
